{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpFL6mGavx9c8rv1HYW7+9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bosunKwak/BigData/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks (RNN)\n",
        "- Finding Structure in Time(1990) 논문\n",
        "\n",
        "### Neural Networks\n",
        "- 뉴런의 구조를 모방해서 구현\n",
        "- Inputs - Weighting - Summation function - Activation function - Outputs\n",
        "- Input layer - Hidden layer - Output layer\n",
        "\n",
        "### Recurrent \n",
        "- 순환하는 구조 \n",
        "\n",
        "### RNN 사용 이유? Time Series Modeling\n",
        "- Time Series = Sequence \n",
        "- 시간 흐름이 있는 데이터를 모델링 하고 싶을 때 RNN사용\n",
        "- Finding Structure in Time(1990)\n",
        "\n",
        "\n",
        "### (1) 시간을 어떻게 반영할까?\n",
        "- Implicit = 처리 과정 중에 시간의 효과를 반영시킴 -> Network with time -> Task dependency\n",
        "- Explicit = 입력값에 명시적인 형태의 값을 반영시킴 -> Spatial representation \n",
        "  - 단점 \n",
        "    - buffer라는 interface안에 입력시 모든 정보를 넣어주어야함\n",
        "    - 고정된 길이의 벡터를 사용해야 되기 때문에 가장 긴 경우의 길이에 맞춰야함 \n",
        "    - 벡터의 유사성을 구하기 어려움 (one-hot vector) \n",
        "\n",
        "### (2) 시간을 반영한다는 것은 무엇인가?\n",
        "- 직전의 결과를 참고하여 다음을 예측 \n",
        "- (1) Jordan : Recurrent connections가 하나의 network memmory 역할 \n",
        "  - output이 input으로 다시\n",
        "- (2) Elman : Context Unit\n",
        "  - output이 아닌 hidden에서 나온 값이 input으로 다시  \n",
        "\n"
      ],
      "metadata": {
        "id": "VRXpbiFEuhiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Encoder-Decoder Approch\n",
        "- Encoder에 LSTM\n",
        "- Contex vector (Encoder->Decoder)\n",
        "\n",
        "- [한계] Context vector가 고정된 길이였기 때문에 긴 문장에 대한 충분한 정보를 담을 수 없음\n",
        "  - 단어가 일정개수 이상 넘어가면 급격히 성능이 저하\n"
      ],
      "metadata": {
        "id": "CJrvjCklujph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Translation by Jointly learning to align and translate(2015)\n",
        "- RNN Encoder-Decoder 의 한계를 해결\n",
        "- Context vector(a single fixed length vector-> a sequence of vector)  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GWlSixdtvaME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[key idea]\n",
        "- decoder에게 무엇이 번역되어야 하는지 말해주는 것\n",
        "- 그 힌트를 네트워크가 학습하게 하는 것  \n",
        "- Learning to align : align(연결선)을 찾아주는 것 -> \"neural network\""
      ],
      "metadata": {
        "id": "G0eRthe0wBN-"
      }
    }
  ]
}