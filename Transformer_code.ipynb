{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMoz5mfCSswCcqtirHlR1wX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bosunKwak/BigData/blob/main/Transformer_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihfeyLnnyPUS"
      },
      "outputs": [],
      "source": [
        "# numpy\n",
        "import numpy as np \n",
        "# pytorch module\n",
        "import torch\n",
        "import torch.nn as nn # torch.nn : architecture를 구성할때 필요한 layer\n",
        "import torch.nn.functional as F # softmax 계산시 사용\n",
        "\n",
        "import math, copy, time\n",
        "# graph\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. EncoderDecoder and Generator models\n",
        "- 최상위 class\n",
        "\n",
        "<img width=\"381\" alt=\"스크린샷 2022-06-09 오후 11 12 57\" src=\"https://user-images.githubusercontent.com/87002218/172868199-3593494b-a543-4f85-9e51-d559f8076f4a.png\">\n"
      ],
      "metadata": {
        "id": "v_m_yOI1ynoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module): #Transformer\n",
        "    '''\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    '''\n",
        "    # Encoder, Decoder, Generator, src_embed(Encoder에 들어가는 input부분의 embedding layer), tgt_embed(decoder로 들어가는 input부분의 embedding layer)\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "    \n",
        "    '''\n",
        "    data관점에서 보면\n",
        "      input -> src_embed-> (data, masking data) -> encoder -> (1)\n",
        "      target input -> tgt_embed -> (data, masking data) -> (2)\n",
        "      (1) & (2) -> decoder -> generator -> output\n",
        "    '''\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask): # src(input), tgt(target input)\n",
        "        memory = self.encode(src, src_mask) \n",
        "        return self.decode(memory, src_mask, tgt, tgt_mask)  # memory: context정보 \n",
        "\n",
        "    ''' 풀어쓰면 아래 코드와 같음\n",
        "    def forward():\n",
        "        x= self.src_embed(src) \n",
        "        x= self.encoder(x, src_mask)\n",
        "        y = self.tgt_embed(tgt)\n",
        "        y = self.decoder(x,src_mask,y,tgt_mask)\n",
        "        return self.generator(y)\n",
        "    '''"
      ],
      "metadata": {
        "id": "7TVY9yulyzOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# layer를 N개 복사하는 함수 \n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\" \n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "8WvxCtKl1D0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder and Decoder\n",
        "- 6개의 sublayer가 있음\n",
        "- Decoder의 입력값:  encoder의 최종결과값, encoder에서 주는 mask 정보, decoder target, target의 mask 정보"
      ],
      "metadata": {
        "id": "6KhQ4WFg2ReH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N): \n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N) #layer를 n번 복사하여 layer list  (n=6)\n",
        "        self.norm = LayerNorm(layer.size) # encoder에서 나온값을 normalization 해 줌 -> 데이터를 고르게 함\n",
        "\n",
        "    ''' Encoder관점에서 보면\n",
        "    x, mask -> encoder -> y,mask\n",
        "    sublayer 6개 통과 \n",
        "    '''    \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers: #layers: layer가 n개\n",
        "            x = layer(x, mask) # layer에 x,mask값이 들어옴 \n",
        "        return self.norm(x) #normalization한 값을 반환"
      ],
      "metadata": {
        "id": "mCFufdKS2vDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "AKoaCQOs3cDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LayerNormalization\n",
        "- batch normalize와는 다름\n",
        "- dimension안에서의 normalization(차원의 값을 맞추어주는 역할)"
      ],
      "metadata": {
        "id": "W5wF6_xU-TkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features_size, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(features_size))\n",
        "        self.shift = nn.Parameter(torch.zeros(features_size))\n",
        "        self.eps = eps\n",
        "        \n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdims=True)\n",
        "        std = x.std(-1, keepdims=True)\n",
        "        return (x - mean) * self.scale / (std + self.eps) + self.shift"
      ],
      "metadata": {
        "id": "OJHITKcP4VPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SubLayerConnection\n",
        "- x -> Normalization (=LayerNorm(x)) -> Sublayer -> x+sublayer(LayerNorm(x))\n",
        "- sublayer에 가기전에 x를 normalization을 해주는 것!! \n",
        "- 일부를 random하게 dropout \n",
        "\n",
        "- SubLayerConnection : Normalization + sublayer"
      ],
      "metadata": {
        "id": "hOMvyUdM41zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SubLayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SubLayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "IUU82QDv4mYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EncoderLayer\n",
        "- self attention layer\n",
        "- Add and Normalization layer(= SubLayerConnection)\n",
        "- Feed forward layer\n",
        "- another Add and Normalization layer"
      ],
      "metadata": {
        "id": "8M62lHyr5wpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__() # 초기화\n",
        "        self.size = size\n",
        "        # attentino layer\n",
        "        self.self_attn = self_attn\n",
        "        # feed forward layer\n",
        "        self.feed_forward = feed_forward\n",
        "        #sublayer 2개 - add and normalization \n",
        "        self.sublayers = clones(SubLayerConnection(size, dropout), 2) # dropout : 얼마나 누락시킬 것인지에 대한 비율\n",
        "        \n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        # x값 3개, mask 값 들어감 \n",
        "        attn_func = lambda x: self.self_attn(x, x, x, mask)\n",
        "        x = self.sublayers[0](x, attn_func) # sublayer 2개를 생성했기때문에 [0]과 [1] \n",
        "        return self.sublayers[1](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "qX6zZnc_6HdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention\n",
        "- Q, K, V 필요\n",
        "- Query : 찾아야할 값 \n",
        "- Key , Value 로 된 DB\n",
        "- Q&K -> MatMul -> Scale(root) -> SoftMax -> (1)\n",
        "- (1)&V -> MatMul -> 결과   \n"
      ],
      "metadata": {
        "id": "2THZ6SaM7WtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropout 은 성능향상을 위해 option으로 넣음 \n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    '''N x a x b 를 matrix shape 라 할때, \n",
        "      query.size(-1) : b를 의미, 즉 차원의 크기 \n",
        "      논문에서는 dimension을 512로 맞춰주고있음 \n",
        "    '''\n",
        "    d_k = query.size(-1) # 차원의 크기 d_k\n",
        "    # key.transpose(-2, -1) : a와 b의 자리를 바꿈 \n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) \n",
        "  \n",
        "    # encoder: 없음 decoder: 있음 \n",
        "    if mask is not None: #mask가 있으면, score에 mask을 매우 작은 값으로 바꿔줌 \n",
        "        scores.masked_fill_(mask == 0, -1e9)\n",
        "    \n",
        "    # torch.nn.function 을 사용 \n",
        "    prob_scores = F.softmax(scores, dim = -1)\n",
        "    \n",
        "    # score에서 일부를 누락시켜 확률 형태의 score값 만듦\n",
        "    \n",
        "    if dropout is not None:\n",
        "        prob_scores = dropout(prob_scores)\n",
        "    \n",
        "    # (1)&V matmul\n",
        "    attn_scores = torch.matmul(prob_scores, value)\n",
        "\n",
        "    return attn_scores, prob_scores"
      ],
      "metadata": {
        "id": "jGVPLvF87WKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiHeadAttention\n",
        "- Query, Key, Value 을 각각 Linear -> Scaled Dot-Product Attention -> Concat -> Linear\n",
        "- scaled dot-product attention 이 h개 쌓여있음 \n",
        "- x data의 shape 변형이 필요함\n",
        "\n",
        "Note that for each attention head, we have an input with shape _something_ like:  \n",
        "몇개의 sentence, 단어의 개수, 단어가 몇 차원으로 embedding했는지 dimension을 나타냄 = 512/8) (h=8)\n",
        "```\n",
        "(batch size, words in each input, 64)\n",
        "```\n",
        "\n",
        "Now that we have multiple heads, we want the input tensor to have _something_ like: (attention heads가 8개) \n",
        "```\n",
        "(batch size, number of attention heads, words in each input, 64)\n",
        "```\n",
        "\n",
        "(3,w,512) -> (3,w,4*128) -> (3,w,4,128) -> (3,4,w,128)   \n",
        "<Concatenate 과정>  \n",
        "-> (3,2,4,128) ->(3,2,4*128) -> (3,w,512)\n"
      ],
      "metadata": {
        "id": "SJDZIEmk9VFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__() # 초기화 \n",
        "        assert d_model % h == 0 # 중요! h는 attention head (논문은 8개)\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h \n",
        "        self.h = h\n",
        "\n",
        "        '''\n",
        "         Query, Key, Value 을 각각 Linear (총 3번)\n",
        "         -> Scaled Dot-Product Attention \n",
        "         -> Concat \n",
        "         -> Linear\n",
        "        ''' # linear layer mapping 총 4번\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4) # d_model : 초창기에 설정한 model \n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Masked 가 있기때문에 mask가 있음\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1) #1번에 해당하는 위치의 차원을 늘려줌 \n",
        "        nbatches = query.size(0) # 0 : batch_size\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        # for 문을 돌며 list 생성 \n",
        "        # zip 함수 : 하나씩 꺼내어 l과 x에 넣어줌 (query -> linear layer를 통해 차원을 바꿔주는 것)\n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        # 8개의 attention에 대한 결과 \n",
        "        # attn_scores와 prob_scores(연관관계에 대한 확률)가 각각 x, self.attn으로 들어감 \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "FDaT1jmH9VQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DecoderLayer\n",
        "- attention layer 2\n",
        "  - masked multi-head attention (mask된 정보 이용)\n",
        "  - multi-head attention(encoder & decoder)\n",
        "- feed-forward 1\n",
        "- sublayerconnection 3"
      ],
      "metadata": {
        "id": "Qisnf6GfPGuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    # attention layer : self_attn, src_attn\n",
        "    # self_attn : masked multi-head attention\n",
        "    # src_attn : multi-head attention \n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size # size : sublayerconnection 에서 단어 임베딩의 크기 (논문에서 512)\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SubLayerConnection(size, dropout), 3)\n",
        " \n",
        "    '''\n",
        "    x : decoder로 들어오는 입력값\n",
        "    tgt_mask : decoder로 들어오는 mask 정보\n",
        "    memory: encoder에서 들어오는 memory\n",
        "    src_mask : encoder에서 들어오는 mask 정보 \n",
        "    '''\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        # 최초 입력값 x와 decoder로 들어오는 입력값이기 때문에 tgt_mask도 필요함 \n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) \n",
        "        # Add&Norm 을 지난 값 x(query), encoder에서 들어오는 m값 2개(key, value)가 sublayer로 들어감 \n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        # 위 결과값이 feedforward layer을 거쳐 결과값 \n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "Wx-1A9ptBY-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mask\n",
        "- 단어를 가려주는 것\n",
        "- RNN 의 recurrent하는 과정을 transformer에서는 mask과정을 사용함으로써 표현\n",
        "- matrix 형태로 구성해서 한번에 처리 가능 \n",
        "- 보이는 정보를 true, 가리는 정보를 false\n"
      ],
      "metadata": {
        "id": "3KDIdt-JQ1b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size): # size : 단어의 개수 \n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size) # 1: batch를 위해 차원을 늘려줌 (size x size)의 틀을 만들어줌 \n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') # np.triu : 남색으로 보이는 부분을 1로 채워주고, 노란색 부분을 0으로 지정\n",
        "    return torch.from_numpy(subsequent_mask) == 0 #(0인것을 참으로 함 - 노란색이 true)\n",
        "plt.imshow(subsequent_mask(15)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "k0S4Y7OVP1yl",
        "outputId": "12cfb093-00c3-46c2-c800-92fae1b32bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f49e164db90>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator\n",
        "decoder에서 나온 결과 -> linear (공간 차원으로 바꿔줌, vocab_size로 늘려줌) -> softmax(확률값으로 변환해줌)\n"
      ],
      "metadata": {
        "id": "3gvQzjHmR9ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab) #d_model : 임베딩한 차원 vocab : 결과값\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1) # dim=-1 : softmax함수를 임베딩 차원으로 계산하겠다는 의미"
      ],
      "metadata": {
        "id": "7Cm7nJfiR9j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FeedForward\n",
        "- Encoder, Decoder에서 사용\n",
        "- 512dim -> 2048 dim -> 512 dim\n",
        "- ReLU function을 사용\n",
        "- 0보다 작으면 0 0보다 크면 linear 함수로 증가 "
      ],
      "metadata": {
        "id": "9RxlB_SZSVAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff) # 512->2048\n",
        "        self.w_2 = nn.Linear(d_ff, d_model) # 2048-> 512 \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x)))) # 1 layer -> relu -> dropout-> 2 layer"
      ],
      "metadata": {
        "id": "19DC5CMKSVHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding\n",
        "- 위치에 대한 정보를 만들어줌\n",
        "- RNN 구조에는 sequence가 진행되는 과정에서 위치에 대한 정보가 자연스럽게 반영되는데, RNN 구조를 사용하지 않으므로 Position encoding이라는 vector를 만들어서 위치정보를 넣어줌\n",
        "- Input -> embedding -> positional encoding -> embedding with time signal\n",
        "- sin함수 : 2i(짝수 위치) , cos 함수 : 2i+1(홀수 위치)"
      ],
      "metadata": {
        "id": "5wpQ4Q1gTGpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_embedding, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout) \n",
        "        \n",
        "        # torch.zeros : 0으로 채워진 tensor를 만듦 \n",
        "        # - max_len : 단어의 개수(position의 최대 값), dim_embedding: dimension의 크기 \n",
        "        positional_encodings = torch.zeros(max_len, dim_embedding)\n",
        "        # positions : 0으로 된 값을 max_len의 크기만큼 만든 배열 (단어의 postion 값)\n",
        "        # 열벡터 값으로 만듦 \n",
        "        positions = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        \n",
        "        # calculate the arguments for sin and cos functions\n",
        "        scale = -(math.log(10000) / dim_embedding) # -log(10^4)/d\n",
        "        arguments = torch.arange(0, dim_embedding, 2).float() * scale \n",
        "        arguments = torch.exp(arguments) \n",
        "        arguments = positions * arguments # sin/cos 함수 넣기 전의 값\n",
        "        \n",
        "        # define the encodings here\n",
        "        positional_encodings[:, 0::2] = torch.sin(arguments) # 짝수\n",
        "        positional_encodings[:, 1::2] = torch.cos(arguments) # 홀수 \n",
        "        \n",
        "        positional_encodings = positional_encodings.unsqueeze(0) # 가장 앞부분의 1 차원을 늘려줌 \n",
        "        self.register_buffer('positional_encodings', positional_encodings) \n",
        "        # register_buffer : position embedding 값은 학습을 하는 값이 아니기 때문에 버퍼를 통해 저장\n",
        "\n",
        "    #\n",
        "    def forward(self, x):\n",
        "        pos_enc = self.positional_encodings[:, :x.size(1)] # x.size(1) : 문장에서의 단어 개수 \n",
        "        pos_enc.requires_grad_(False) #gradient를 구하지 않음 == 학습을 하지 않겠다 \n",
        "        x  = x + pos_enc\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "5KTZxosITG0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "- 단어를 모아 vocabulary 를 만들고 vocab을 갖고 분산된 표현으로 단어를 바꾸어줌 "
      ],
      "metadata": {
        "id": "tgzlSNn4VfCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab, dim_embedding):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab, dim_embedding) # (vocab set, 몇 차원으로 표현할지)\n",
        "        self.dim_embedding = dim_embedding\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # embedding is multiplied by scale to make the positional encoding relatively smaller\n",
        "        # See: https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n",
        "        return self.embed(x) * math.sqrt(self.dim_embedding) # scale\n",
        "    "
      ],
      "metadata": {
        "id": "Q3UQvrLWVgfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Model "
      ],
      "metadata": {
        "id": "yaKUhmhkV7ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(src_vocab, tgt_vocab, \n",
        "               num_enc_dec=6, dim_model=512, dim_feedfwd=2048, attn_heads=8, dropout=0.1):\n",
        "    # prepare the embeddings for encoder and decoder stacks\n",
        "    position_embeddings = PositionalEncoding(dim_model, dropout)\n",
        "    src_embed = nn.Sequential(Embedding(src_vocab, dim_model), copy.deepcopy(position_embeddings))\n",
        "    tgt_embed = nn.Sequential(Embedding(tgt_vocab, dim_model), copy.deepcopy(position_embeddings))\n",
        "    \n",
        "    # prepare reusable layers. we will copy.deepcopy them whenever needed\n",
        "    attn_layer = MultiHeadedAttention(attn_heads, dim_model)\n",
        "    feed_fwd_layer = PositionwiseFeedForward(dim_model, dim_feedfwd, dropout)\n",
        "    c = copy.deepcopy\n",
        "    \n",
        "    # prepare the encoder stack\n",
        "    encoder_layer = EncoderLayer(dim_model, c(attn_layer), c(feed_fwd_layer), dropout)\n",
        "    encoder = Encoder(encoder_layer, num_enc_dec)\n",
        "    \n",
        "    # prepare the decoder stack\n",
        "    decoder_layer = DecoderLayer(dim_model, c(attn_layer), c(attn_layer), c(feed_fwd_layer), dropout)\n",
        "    decoder = Decoder(decoder_layer, num_enc_dec)\n",
        "    \n",
        "    # prepare the generator\n",
        "    generator = Generator(dim_model, tgt_vocab)\n",
        "    \n",
        "    # create the model\n",
        "    model = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)\n",
        "    \n",
        "    # Initialize parameters using Xavier initialization\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "            \n",
        "    return model"
      ],
      "metadata": {
        "id": "e_dvGMQhV6-W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}