{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOk7uxHV/pfb60KF+63YU1M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bosunKwak/BigData/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "### Attention is All you Need (2017)\n",
        "RNN 대신 attention mechanism -> 계산 효율성 증가, 병렬 처리 가능\n",
        "- Recurrent neural networks(RNN) : long short-term memory and gated recurrent neural networks\n",
        "- sequence modeling\n",
        "- encoder-decoder architectures \n",
        "- eschewing recurrence and instead relying entirely on an attention mechanism\n",
        "\n",
        "Input(sequence: 단어, 이미지, 등) -> Transformer Architecture -> Output"
      ],
      "metadata": {
        "id": "8bD37ahVkOUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder and Decoder Stacks\n",
        "input -> encoder1 -> encoder2 ->...-> encoder6 ->(문맥정보가 잘 담긴 정보가 추출됨- Q,K,V가 나오게되는데 K,V를 decoder에 활용하게 됨)-> decoder1 -> decoder2 -> ... -> decoder(n=6 일 때)\n",
        "\n",
        "#### Encoder module\n",
        "- Input data에서 어떤 정보를 뽑아내는 것\n",
        "1. self-attention\n",
        "2. feed forward\n",
        "\n",
        "#### Decoder\n",
        "- Encoder에서 전달받은 정보(State)를 가지고 Output을 만들어내는 과정\n",
        "1. self-attention\n",
        "2. encoder-decoder attention (encoder와 decoder가 만나는 지점)\n",
        "3. feed forward\n",
        "\n",
        "- State == Context(Sequence에 대한 정보를 가지고 있음)\n",
        "\n",
        "<img width=\"400\" alt=\"스크린샷 2022-06-09 오후 10 21 26\" src=\"https://user-images.githubusercontent.com/87002218/172857197-bc943f87-74fe-4654-bca4-153f8fb94df2.png\">\n",
        "\n",
        "- 빨간색 : encoder , 파란색 : decoder \n",
        "\n"
      ],
      "metadata": {
        "id": "Mt2BjcjTkRUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Self attention\n",
        "- attention mechanism \n",
        "- sequence의 representation을 구하는 것\n",
        "- Attention(Q,K,V) = softmax(QKᵀ/root dₖ)V  \n",
        "  (cf. Query(Q): 찾고 싶은 값, Key(K): 색인, Value(V): 실제값)  \n",
        "  Q와 K MatMul -> Scale -> SoftMAx -> (1)  \n",
        "  (1)와 V MatMul -> 결과   \n",
        "  - 개념공간(단어 임베딩 차원)을 Query차원, Key차원, Value 차원으로 표현\n",
        "<img width=\"800\" alt=\"스크린샷 2022-06-09 오후 9 57 45\" src=\"https://user-images.githubusercontent.com/87002218/172853402-61b360c1-d290-4e1e-871a-bede609087ea.png\">\n"
      ],
      "metadata": {
        "id": "OC2WqnNIopNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention\n",
        "- Encoder : Multi-Head Attention(input값으로만)\n",
        "- Decoder : Multi-Head Attention(K,V,Q'), Masked Multi-Head Attention(output 값으로만)\n",
        "\n",
        "#### Multi-Head Attention\n",
        "- Scaled dot-Product attention를 논문에서는 8개 묶어서 사용\n",
        "- cf. 위에서 설명한 attention을 scaled dot-product attention이라함\n",
        "\n",
        "#### Masked Multi-Head Attention\n",
        "- 단어를 일부러 가리고, 예측 해야 할 값을 Label로 둠\n",
        "- masking이 되어있는 부분에서 attention을 찾음"
      ],
      "metadata": {
        "id": "rPwBJORmoqfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed-Forward Networks\n",
        "- 512 dim -> 2048 dim -> 512 dim\n",
        "- Word2Vec \n",
        "- value차원의 개념공간에 Query와 Key의 연관성 반영 -> \"레이어를 거칠수록 문맥 정보가 더 반영\" -> 연관성(문맥)이 반영된 단어 간 관계로 재구성\n",
        "\n"
      ],
      "metadata": {
        "id": "YrK0hyCVqfzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residuals\n",
        "- Add & Normalize\n",
        "- LayerNorm(X+Z)\n"
      ],
      "metadata": {
        "id": "-4edEunrrCmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear & Softmax\n",
        "512 -> Linear를 통해 확장 -> Softmax을 거쳐 확률 값 계산"
      ],
      "metadata": {
        "id": "wpFnwjf_ra2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why self-attention\n",
        "- total computational complexity per layer 이 줄어들음\n",
        "- computation can be parallized\n",
        "- 문장이 길어져도 단어간의 문맥 반영 (long-range dependencies)\n",
        "- more interpretable model\n"
      ],
      "metadata": {
        "id": "PwSZ08-prvzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout\n",
        "- sub layer를 더하기 전\n",
        "- embedding을 해 positional encoding을 더할때 \n",
        "- 비율을 0.1로 "
      ],
      "metadata": {
        "id": "M-SjYjh699wj"
      }
    }
  ]
}